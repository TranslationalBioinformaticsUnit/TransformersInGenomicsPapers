# Transformers In Genomics Papers
A curated repository of academic papers showcasing the use of Transformer models in genomics. This repository aims to guide researchers, data scientists, and enthusiasts in finding relevant literature and understanding the applications of Transformers in various genomic contexts.

## Summary Statistics

| Data Type                             | Original Papers | Benchmarking Papers | Review/Perspective Papers |
|---------------------------------------|----------------:|--------------------:|--------------------------:|
| Single-Cell Genomics (SCG)            |               39|                    4|                         1 |
| DNA                                   |               31|                    1|                         2 |
| Spatial Transcriptomics (ST)          |               11|                    0|                         0 |
| Hybrid of SCG, DNA, and ST            |                8|                    0|                         0 |

## Table of Contents

1. [Single-Cell Genomics (SCG) Models](#single-cell-genomics-scg-models)
   - [Original Papers](#original-papers)
   - [Benchmarking Papers](#benchmarking-papers)
   - [Review/Perspective Papers](#reviewperspective-papers)

2. [DNA Models](#dna-models)
   - [Original Papers](#original-papers-1)
   - [Benchmarking Papers](#benchmarking-papers-1)
   - [Review/Perspective Papers](#reviewperspective-papers-1)

3. [Spatial Transcriptomics (ST) Models](#spatial-transcriptomics-st-models)
   - [Original Papers](#original-papers-2)
   - [Benchmarking Papers](#benchmarking-papers-2)
   - [Review/Perspective Papers](#reviewperspective-papers-2)

4. [Hybrids of SCG, DNA, and ST Models](#hybrids-of-scg-dna-and-st-models)
   - [Original Papers](#original-papers-3)
   - [Benchmarking Papers](#benchmarking-papers-3)
   - [Review/Perspective Papers](#reviewperspective-papers-3)

#### Legend
* üí°: Pretrained Model
* üîç: Peer-reviewed
## Single-Cell Genomics (SCG) Models
Papers that utilize Transformer models to analyze single-cell genomic data.

### Original Papers
| üß† Model        | üìÑ Paper           | üíª Code | üõ†Ô∏è Architecture        | üåü Highlights/Main Focus                         | üß¨ No. of Cells | üìä No. of Datasets | üéØ Loss Function(s)       | üìù Downstream Tasks/Evaluations        |
|-----------------|---------------------|---------|--------------------------|-------------------------------------------------|----------------|-------------------|--------------------------|---------------------------------------|
| scFoundationüí°üîç   | [Large-scale foundation model on single-cell transcriptomics](https://www.nature.com/articles/s41592-024-02305-7). Minsheng Hao et al. _Nature Methods_ (2024)         | [GitHub Repository](https://github.com/biomap-research/scFoundation)        | Transformer encoder, Performer decoder | Foundation model for single-cell analysis, built on xTrimoGene architecture with a read-depth-aware (RDA) pretraining across 50 million profiles | 50M | 7 | Mean square error loss | Cell clustering; Cell type annotation; Perturbation prediction; Drug response prediction |
| scGPT üí°üîç          | [scGPT: toward building a foundation model for single-cell multi-omics using generative AI](https://www.nature.com/articles/s41592-024-02201-0). Haotian Cui et al. _Nature Methods_ (2024) |  [GitHub Repository](https://github.com/bowang-lab/scGPT)       | Transformer | A foundation model designed for single-cell multi-omics aimed to deepen the understanding of biological data and improve performance in tasks like cell type annotation and integration. | 33M | 441 | Mean square error; Cosine similarity; Cross entropy loss | Cell type annotation; Perturbation response prediction; Multi-batch integration; Multi-omic integration; Gene regulatory network inference |
| MarsGT üîç         | [MarsGT: Multi-omics analysis for rare population inference using single-cell graph transformer](https://www.nature.com/articles/s41467-023-44570-8). Xiaoying Wang et al. _Nature Communications_ (2024) | [GitHub Repository](https://github.com/OSU-BMBL/marsgt)        | Graph Transformer | Identifying rare cell populations in single-cell multi-omics, with superior performance and insights for early detection and therapeutic intervention strategies | 750K | 550 | KL divergence, cosign similarity, and regression loss | Construct enhancer gene regulatory networks |
| scGREAT üîç        | [scGREAT: Transformer-based deep-language model for gene regulatory network inference from single-cell transcriptomics](https://www.sciencedirect.com/science/article/pii/S258900422400573X). Yuchen Wang et al. _iScience_ (2024) | [GitHub Repository](https://github.com/ChaozhongLiu/scGREAT?tab=readme-ov-file)        | Transformer | Inferencing Gene Regulatory Networks (GRN) from single-cell transcriptomics data and textual information about genes using a transformer-based model | 4K | 7 | Cross entropy loss | Gene Regulatory Network Inference |
| scMulan üí°üîç         | [scMulan: A Multitask Generative Pre-Trained Language Model for Single-Cell Analysis](https://link.springer.com/chapter/10.1007/978-1-0716-3989-4_57). Haiyang Bian et al. _Research in Computational Molecular Biology (RECOMB)_ (2024) | [GitHub Repository](https://github.com/SuperBianC/scMulan)        | Transformer | Generative multitask model for single-cell analysis, trained on 10 million cells. | 10M | 5 | Cross entropy loss | Cell type annotation; Batch integration; Conditional cell generation |
| CellPLM üí°üîç        | [CellPLM: Pre-training of Cell Language Model Beyond Single Cells](https://openreview.net/pdf?id=BKXvPDekud). Hongzhi Wen et al. _International Conference on Learning Representations (ICLR)_ (2024) |       [GitHub Repository](https://github.com/OmicsML/CellPLM)   | Transformer | The framework marks the first of its kind, encoding inter-cell relations, harnessing spatially-resolved transcriptomic data, and adopts a decent prior distribution. | 9M scRNA-seq + 2M spatial | 3 | Masked language modeling with mean squared error loss | Zero-shot clustering; scRNA-seq denoising; Spatial transcriptomic imputation; Cell type annotation; Perturbation prediction |
| tGPT üí°üîç           | [Generative pretraining from large-scale transcriptomes for single-cell deciphering](https://www.sciencedirect.com/science/article/pii/S2589004223006132). Hongru Shen et al. _iScience_ (2023) | [GitHub Repository](https://github.com/deeplearningplus/tGPT)        | Transformer | Generative pretraining on 22.3 million single-cell transcriptomes aligns with established cell labels and states suitable for single-cell and bulk analysis. | 22.3M | 4 | Cross entropy loss | Single-cell clustering; Inference of developmental lineage; Feature representation analysis of bulk tissues |
| TOSICA üîç         | [Transformer for one stop interpretable cell type annotation](https://www.nature.com/articles/s41467-023-35923-4). Jiawei Chen et al. _Nature Communications_ (2023) | [GitHub Repository](https://github.com/JackieHanLab/TOSICA)        | Transformer | An efficient cell type annotator trained on scRNA-seq data shows high accuracy across diverse datasets and enables new cell type discovery. | 536K | 6 | Cross entropy loss | Cell type annotation; Data integration; Cell differentiation trajectory inference |
| Geneformer üí°üîç     | [Transfer learning enables predictions in network biology](https://www.nature.com/articles/s41586-023-06139-9). Christina V. Theodoris et al. _Nature_ (2023) | [Hugging Face Repository](https://huggingface.co/ctheodoris/Geneformer); [GitHub Repository](https://github.com/jkobject/geneformer)        | Transformer | Pre-trained on 30 million single-cell transcriptomes to enable context-specific predictions and identify therapeutic targets in network biology with limited data. | 30M | 561 | Cross entropy loss | Chromatin dynamics prediction; Network dynamics prediction; Cell type annotation; Gene network analysis |
| STGRNS üîç         | [STGRNS: an interpretable transformer-based method for inferring gene regulatory networks from single-cell transcriptomic data](https://academic.oup.com/bioinformatics/article/39/4/btad165/7099621). Jing Xu et al. _Bioinformatics_ (2023) | [GitHub Repository](https://github.com/zhanglab-wbgcas/STGRNS)        | Transformer | Focused on enhancing gene regulatory network inference from single-cell transcriptomic data using a proposed gene expression motif technique, applicable across various scRNA-seq data types. | 154K+ | 48 | Cross entropy loss | Gene regulatory networks inference |
| DeepMAPS üîç       | [Single-cell biological network inference using a heterogeneous graph transformer](https://www.nature.com/articles/s41467-023-36559-0). Anjun Ma et al. _Nature Communications_ (2023) |  [GitHub Repository](https://github.com/OSU-BMBL/deepmaps)       | Graph Transformer | Infers biological networks from single-cell multi-omics data via a heterogeneous graph and a multi-head graph transformer, enhancing local and global context learning. | 199K | 17 | Mean squared error and KL divergence | Dimensionality reduction and cell clustering; Biological network construction |
| scBERT üí°üîç         | [scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data](https://www.nature.com/articles/s42256-022-00534-z). Fan Yang et al. _Nature Machine Intelligence_ (2022) | [GitHub Repository](https://github.com/TencentAILabHealthcare/scBERT)        | Transformer (BERT-based model) | A BERT-based model was pre-trained on large amounts of unlabeled scRNA-seq data for cell type annotation, demonstrating superior performance. | 1M | 10 | Cross entropy loss | Cell type annotation; Novel cell type prediction |
| scCLIP üí°üîç         | [scCLIP: Multi-modal Single-cell Contrastive Learning Integration Pre-training](https://openreview.net/pdf?id=KMtM5ZHxct). Lei Xiong et al. _Conference on Neural Information Processing Systems (NeurIPS) AI for Science Workshop_ (2023) | [GitHub Repository](https://github.com/jsxlei/scCLIP)        | Transformer | Introduced a multi-modal Transformer model with contrastive learning, optimized for single-cell ATAC-seq data by tokenizing genomic peaks | 377K | 2 | Cross entropy loss | Modality alignment |
| scMVP üîç          | [A deep generative model for multi-view profiling of single-cell RNA-seq and ATAC-seq data](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02595-6). Gaoyang Li et al. _Genome Biology_ (2022) | [GitHub Repository](https://github.com/bm2-lab/scMVP)        | Transformer + VAE | Introduces scMVP, a multi-modal deep generative model for processing single-cell RNA-seq and ATAC-seq data, addressing data sparsity and integration challenges. | 100K | 5 | Clustering consistency loss ‚Äì similar to CycleGAN | Clustering; Imputation; Trajectory Inference |
| Enformer üîç       | [Effective gene expression prediction from sequence by integrating long-range interactions](https://www.nature.com/articles/s41592-021-01252-x). ≈Ωiga Avsec et al. _Nature Methods_ (2021) | [Hugging Face Repository](https://huggingface.co/EleutherAI/enformer-preview); [GitHub Repository](https://github.com/lucidrains/enformer-pytorch)     | Transformer with attention layers | To improve gene expression prediction from DNA sequences by integrating long-range interactions, leveraging transformer architecture for better accuracy. | 254K | 2 | Poisson negative log-likelihood loss | Gene expression prediction; Variant effect prediction; Epigenetic state prediction |
| CIForm üîç               | [CIForm as a Transformer-based model for cell-type annotation of large-scale single-cell RNA-seq data](https://academic.oup.com/bib/article/24/4/bbad195/7169137). Jing Xu et al. _Briefings in Bioinformatics_ (2023)    | [GitHub Repository](https://github.com/zhanglab-wbgcas/CIForm)       | Transformer               | Developed for cell-type annotation of large-scale single-cell RNA-seq data, aiming to overcome batch effects and efficiently process large datasets | 12M      | 16                | Cross entropy loss       | Cell type annotation                  |
| TransCluster üîç         | [TransCluster: A Cell-Type Identification Method for single-cell RNA-Seq data using deep learning based on transformer](https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2022.1038919/full). Tao Song et al. _Frontiers Genetics_ (2022)          |  [GitHub Repository](https://github.com/Danica123/TransCluster)                    | Transformer               | Proposes TransCluster, combining linear discriminant analysis and a modified Transformer to enhance cell-type identification accuracy and robustness across various human tissue datasets | 51K             | 2                 | Cross entropy loss       | Cell type annotation                  |
| iSEEEK üí°üîç               | [A universal approach for integrating super large-scale single-cell transcriptomes by exploring gene rankings](https://pubmed.ncbi.nlm.nih.gov/35048121/). Hongru Shen et al. _Briefings in Bioinformatics_ (2022)      |  [GitHub Repository](https://github.com/lixiangchun/iSEEEK)     | Transformer               | Introduces iSEEEK, an approach for integrating super large-scale single-cell RNA sequencing data by exploring gene rankings of top-expressing genes and states suitable for single-cell and bulk analysis | 11.9M           | 60                | Cross entropy loss       | Cell clusters delineation; Marker genes identification; Cell developmental trajectory exploration; Cluster-specific gene-gene interaction modules exploration analysis of bulk tissues |
| Exceiver üí°             | [A single-cell gene expression language model](https://arxiv.org/abs/2210.14330). Connell et al. _arXiv_ (2022)                              |  [GitHub Repository](https://github.com/keiserlab/exceiver)                    | Transformer               | Introduced discrete noise masking for self-supervised learning on unlabeled datasets and developed a framework using scRNA-seq to enhance downstream tasks in gene regulation and phenotype prediction | 500K            | 1                 | Cross entropy loss + Mean square error | Drug response prediction              |
| xTrimoGene üí°üîç           | [xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data](https://papers.nips.cc/paper_files/paper/2023/file/db68f1c25678f72561ab7c97ce15d912-Paper-Conference.pdf). Jing Gong et al. _Conference on Neural Information Processing Systems (NeurIPS)_ (2023) |  _Unpublished_    | Asymmetric encoder-decoder transformer | Introduced a transformer variant for scRNA-seq data, significantly reducing computational and memory usage while preserving accuracy, and developed tailored pre-trained models for single-cell data | 5M       | -                 | Mean square error         | Cell type annotation; Perturbation response prediction; Synergistic drug combination prediction |
| Cell2Sentence üí°üîç        | [Cell2Sentence: Teaching Large Language Models the Language of Biology](https://www.biorxiv.org/content/10.1101/2023.09.11.557287v3). Daniel Levine et al. _International Conference on Machine Learning (ICLR)_ (2024)                        |  [GitHub Repository](https://github.com/vandijklab/cell2sentence-ft)                    | Transformer (GPT)         | A single and flexible framework for seamlessly integrating Large Language Models (LLMs), specifically GPT-2, into transcriptomics, leveraging widely-used LLM libraries | 40K              | 2                 | Cross entropy loss       | Unconditional cell generation; Conditional cell generation; Cell type prediction |
| GenePT üí°               | [GenePT: A Simple But Effective Foundation Model for Genes and Cells Built From ChatGPT](https://www.biorxiv.org/content/10.1101/2023.10.16.562533v2). Yiqun T. Chen & James Zou _bioRxiv_ (2023)       |  [GitHub Repository](https://github.com/yiqunchen/GenePT)                        | Transformer (GPT)         | Used NCBI text descriptions of individual genes with GPT-3.5 to generate gene embeddings then further leveraged on downstream tasks | 21K             | 10                | Cross entropy loss       | Gene property prediction; Batch integration; Cell type annotation |
| CellLM üí°               | [Large-Scale Cell Representation Learning via Divide-and-Conquer Contrastive Learning](https://arxiv.org/abs/2306.04371). Suyuan Zhao et al. _arXiv_ (2023)    |  [GitHub Repository](https://github.com/PharMolix/OpenBioMed)                | Performer Transformer     | Presented a novel divide-and-conquer contrastive learning strategy designed to decouple the batch size from GPU memory constraints in cell representation learning | 2M               | 2                 | Masked language modeling with cross-entropy loss, cell type discrimination with binary cross-entropy loss, and divide-and-conquer contrastive loss | Cell type annotation; Drug sensitivity prediction |
| scELMo üí°               | [scELMo: Embeddings from Language Models are Good Learners for Single-cell Data Analysis](https://www.biorxiv.org/content/10.1101/2023.12.07.569910v2). Tianyu Liu et al. _bioRxiv_ (2023)    | [GitHub Repository](https://github.com/HelloWorldLTY/scELMo)                     | Transformer (GPT)         | Extended the concept from GenePT and proposed a novel approach to leverage the advantages from Large Language Models (LLMs) to formalize a foundation model for single-cell data analysis | 69K              | 5                 | Cross entropy loss       | Cell clustering; Batch effect correction; Cell type annotation; Perturbation analysis |
| UCE üí°                  | [Universal Cell Embeddings: A Foundation Model for Cell Biology](https://www.biorxiv.org/content/10.1101/2023.11.28.568918v1). Yanay Rosen et al. _bioRxiv_ (2023)     | [GitHub Repository](https://github.com/snap-stanford/UCE)                     | Transformer          | Trained in a self-supervised learning fashion on a diverse corpus of cell atlas data encompassing humans and other species, this model offers a cohesive biological latent space capable of representing cells from any tissue or species, all without the need for manual data annotations | 36M              | 300               | Cross entropy loss       | Zero-shot embedding quality and clustering; Cell type organization; Zero-shot cell type alignment to Integrated Mega-scale Atlas (IMA) |
| CellFM üí°                  | [a large-scale foundation model pre-trained on transcriptomics of 100 million human cells](https://www.biorxiv.org/content/10.1101/2024.06.04.597369v1). Yuansong Zeng et al. _bioRxiv_ (2024)     | [GitHub Repository](https://github.com/biomedAI/CellFM)                     | Transformer          | A 800-million-parameter single-cell model trained on ~100 million human cells, outperforming existing models in applications like cell annotation and gene function prediction | 100M              | 20               | Mean square error loss loss       |Cell type annotation; Pertubation prediction; Gene function predction |
| Nicheformer üí°           | [Nicheformer: A Foundation Model for Single-Cell and Spatial Omics](https://www.biorxiv.org/content/10.1101/2024.04.15.589472v1). Anna C. Schaar et al. _bioRxiv_ (2024)    | [GitHub Repository](https://github.com/theislab/nicheformer)                     | Transformer               | Transformer-based model that integrates over 110 million human and mouse cells, learning unified representations from dissociated and spatial transcriptomics for advanced analysis of cellular interactions and environments. | 110M     | 180+      | Masked language modeling loss, Cross entropy loss | Spatial cell type, niche region label prediction; Neighborhood cell density prediction |
| CELLama üí°                | [CELLama: Foundation Model for Single Cell and Spatial Transcriptomics by Cell Embedding Leveraging Language Model Abilities](https://www.biorxiv.org/content/10.1101/2024.05.08.593094v1). Hongyoon Choi et al. _bioRxiv_ (2024)         | [GitHub Repository](https://github.com/portrai-io/CELLama)                     | Transformer               | CELLama leverages language models to transform scRNA-seq and spatial transcriptomics data into gene expression 'sentences,' facilitating advanced cellular analysis across diverse datasets | 536K+           | 4                 | Cosine similarity         | Cell typing; Integration              |
| LangCell üí°              | [LangCell: Language-Cell Pre-training for Cell Identity Understanding](https://arxiv.org/abs/2405.06708). Suyuan Zhao et al. _arXiv_ (2024)                          | [GitHub Repository](https://github.com/PharMolix/LangCell)                     |   Transformer                        | LangCell integrates single-cell data with natural language during pre-training enabling effective zero-shot, few-shot, and fine-tuning performance in cell identity understanding tasks | 27.5M    | 4                 | Masked gene modeling, Cell-cell contrastive, Cell-text contrastive, and Cell-text matching losses | Novel cell type identification; Cell type annotation; Batch integration |
| GeneCompass üí°           | [GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model](https://www.biorxiv.org/content/10.1101/2023.09.26.559542v1). Xiaodong Yang et al. _bioRxiv_ (2023)                        |  [GitHub Repository](https://github.com/xCompass-AI/GeneCompass)                    |     Transformer                      | Cross-species foundation model pre-trained on over 120 million single-cell transcriptomes from humans and mice, integrating biological prior knowledge | 120M     | 13                | Mean square error, Cross entropy loss | Cell type annotation; Gene regulatory network prediction; Drug dose response prediction |
| scTranslator üí°          | [A pre-trained large generative model for translating single-cell transcriptome to proteome](https://www.biorxiv.org/content/10.1101/2023.07.04.547619v2). Linjing Liu et al. _bioRxiv_ (2023)                          |  [GitHub Repository](https://github.com/TencentAILabHealthcare/scTranslator)                    | Transformer               | scTranslator, a pre-trained generative model inspired by NLP and genetic translation, enhances single-cell proteomics by generating multi-omics data from the transcriptome | 239K           | 76                | Mean square error         | Interaction inference; Cell clustering |
| scMoFormer üîç            | [Single-Cell Multimodal Prediction via Transformers](https://dl.acm.org/doi/10.1145/3583780.3615061). Wenzhuo Tang et al. _ACM International Conference on Information and Knowledge Management (CIKM)_ (2023)                         |  [GitHub Repository](https://github.com/OmicsML/scMoFormer)                    | Transformer               | Transformer-based framework designed to leverage and model the interactions of multimodal single-cell data, incorporating external domain knowledge for enhanced performance | 146K        | 3                 | Mean square error loss    | Multimodal prediction                |
| scTransSort üí°üîç           | [scTransSort: Transformers for Intelligent Annotation of Cell Types by Gene Embeddings](https://www.mdpi.com/2218-273X/13/4/611). Linfang Jiao et al. _Biomolecules_ (2023)                         |  [GitHub Repository](https://github.com/jiaojiao-123/scTransSort)                    | Transformer               | Cell-type annotation using transformers, pre-trained on single-cell transcriptomics data | 185K            | 47                | Sparse Categorical Cross entropy | Cell type annotation                  |
| BioFormers              | [BioFormers: A Scalable Framework for Exploring Biostates using Transformers](https://www.biorxiv.org/content/10.1101/2023.11.29.569320v1). Siham Amara-Belgadi et al. _bioRxiv_ (2023)                          |  [GitHub Repository](https://github.com/BiostateAI/Bioformers-BERT)                     | Transformer               | Transformer-based unsupervised learning to model biological systems, defining a 'biostate' as a comprehensive vector of genomic, proteomic, and other biological markers | 8K            | 3                 | Cross entropy loss         | Genetic perturbation prediction; Gene network inference |
| MuSe-GNN üîç              | [MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological Graph Data](https://openreview.net/pdf?id=4UCktT9XZx). Tian Yu et al. _Conference on Neural Information Processing Systems (NeurIPS)_ (2023)         | [GitHub Repository](https://github.com/HelloWorldLTY/MuSe-GNN)                     | Graph-Transformer         | Multimodal Similarity Learning Graph Neural Network, for integrating multimodal biological data to uncover gene function similarities across diverse datasets | -                | 82                | Binary cross entropy, Cosine similarity, Noise contrastive estimation loss | Cell clusters delineation; Marker genes identification; Cell developmental trajectory exploration; Cluster-specific gene‚Äìgene interaction modules exploration analysis of bulk tissues |
| scFormer               | [scFormer: A Universal Representation Learning Approach for Single-Cell Data Using Transformers](https://www.biorxiv.org/content/10.1101/2022.11.20.517285v1). Haotian Cui et al. _bioRxiv_ (2022)                          | [GitHub Repository](https://github.com/bowang-lab/scFormer)                     | Transformer               | Transformer-based deep learning framework employing self-attention to jointly optimize unsupervised cell and gene embeddings | 27K             | 3                 | Cross entropy loss         | Integration; Perturbation prediction |
| scTT üîç                  | [Representation Learning and Translation between the Mouse and Human Brain using a Deep Transformer Architecture](https://icml-compbio.github.io/icml-website-2020/2020/papers/WCBICML2020_paper_29.pdf). Minxing Pang & Jesper Tegn√©r. _International Conference on Machine Learning (ICML) Workshop on Computational Biology_ (2020)                              | _Unpublished_                     | Transformer               | Transformer-based architecture translates single-cell genomic data between mouse and human, with enhanced clustering accuracy | 170K             | 2                 | Mean square error          | Clustering; Alignment                 |
| scmFormer üí°üîç                  | [scmFormer Integrates Large-Scale Single-Cell Proteomics and Transcriptomics Data by Multi-Task Transformer](https://pubmed.ncbi.nlm.nih.gov/38483032/). Jing Xu et al. _Advanced Science_ (2024)                              | _Unpublished_                     | Transformer  decoder             | Transformer-based model integrating single-cell multi-omics data outperforming existing methods in label transfer and handling large-scale datasets. It also improves modality generation and spatial multi-omic analysis. | 1.48M             | 24                | Mean square error          | Missing modality generation; Missing features generation; Cell type label transfer; Clustering; Dimentionality reduction                 |
| scPRINT üí°                 | [scPRINT: pre-training on 50 million cells allows robust gene network predictions](https://www.biorxiv.org/content/10.1101/2024.07.29.605556v1). J√©r√©mie Kalfon et al. _bioRxiv_ (2024)                              | [GitHub Repository](https://github.com/cantinilab/scPRINT)                   | Transformer            | A large transformer-based cell model pre-trained on over 50 million cells and designed to infer gene networks and uncover complex cellular biology. | 50M+           | 800+          | A combination of negative log-likelihood loss and contrastive loss  | Gene network inference         |




### Benchmarking Papers
| üìÑ Paper                                          | üíª Code              | üß† Benchmarking Models           | üåü Main Focus                          | üìù Results & Insights |
|---------------------------------------------------|----------------------|----------------------------------|----------------------------------------|-----------------------|
| [Evaluating the Utilities of Foundation Models in Single-cell Data Analysis](https://www.biorxiv.org/content/10.1101/2023.09.08.555192v5). Tianyu Liu et al. _bioRxiv_ (2024)   |  [GitHub Repository](https://github.com/HelloWorldLTY/scEval)   |  scGPT, scFoundation, tGPT, GeneCompass, SCimilarity, UCE, and CellPLM | This paper evaluates the performance of foundation models (FMs) in single-cell sequencing data analysis, comparing them to task-specific methods across eight downstream tasks and proposing a systematic evaluation framework (scEval) for training and fine-tuning single-cell FMs. The study highlights that while single-cell FMs may not always outperform task-specific methods, they show promise in cross-species/cross-modality transfer learning and possess unique emergent abilities.    | Open-source single-cell FMs generally outperform closed-source ones due to their accessibility and the community feedback they receive; pre-training significantly enhances model performance in tasks like Cell-type Annotation and Gene Function Prediction. However, the study also found limitations in the stability and performance of single-cell FMs across certain tasks, suggesting the need for more nuanced training and fine-tuning processes, and indicating substantial room for improvement in their development. |
| [Foundation Models Meet Imbalanced Single-Cell Data When Learning Cell Type Annotations](https://www.biorxiv.org/content/10.1101/2023.10.24.563625v1). Abdel Rahman Alsabbagh et al. _bioRxiv_ (2023)  | [GitHub Repository](https://github.com/SabbaghCodes/ImbalancedLearningForSingleCellFoundationModels)   | scGPT, scBERT, and Geneformer |  The paper focuses on evaluating the performance of three single-cell foundation models‚ÄîscGPT, scBERT, and Geneformer‚Äîwhen trained on datasets with imbalanced cell-type distributions. It explores how these models handle skewed data distributions, particularly in the context of cell-type annotation.  | scGPT and scBERT perform comparably well in cell-type annotation tasks, while Geneformer lags presumably due to its unique gene tokenization method, with all models benefiting from random oversampling to address data imbalances. Additionally, scGPT offers the fastest computational speed using FlashAttention, whereas scBERT is the most memory-efficient, highlighting trade-offs between speed and memory usage in these foundation models. The paper suggests that future directions should explore enhanced data representation strategies and algorithmic innovations, including tokenization and sampling techniques, to further mitigate imbalanced learning challenges in single-cell foundation models, aiming to improve their robustness across diverse biological datasets.  |
| [Reusability report: Learning the transcriptional grammar in single-cell RNA-sequencing data using transformers](https://www.nature.com/articles/s42256-023-00757-8). Sumeer Ahmad Khan et al. _Nature Machine Intelligence_ (2023)      | [GitHub Repository](https://github.com/TranslationalBioinformaticsUnit/scbert-reusability)     | scBERT | This paper focuses on evaluating the reusability and generalizability of the scBERT method, originally designed for cell-type annotation in single-cell RNA-sequencing data, beyond its initial datasets. It highlights the significant impact of cell-type distribution on scBERT's performance and introduces a subsampling technique to mitigate imbalanced data distribution, offering insights for optimizing transformer models in single-cell genomics.    | While scBERT can reproduce the main results in cell-type annotation, its performance is significantly affected by the distribution of cells per cell type, particularly struggling with novel cell types in imbalanced datasets. Addressing this distributional sensitivity is crucial, suggesting future work should focus on developing methods to handle class imbalance and leveraging domain knowledge to enhance transformer models in single-cell genomics. |
| [Assessing the limits of zero-shot foundation models in single-cell biology](https://www.biorxiv.org/content/10.1101/2023.10.16.561085v2). Kasia Z. Kedzierska et al. _bioRxiv_ (2023) | [GitHub Repository](https://github.com/microsoft/zero-shot-scfoundation)    | Geneformer and scGPT  | The main focus of this paper is to rigorously evaluate the zero-shot performance of foundation models, specifically Geneformer and scGPT, in single-cell biology to determine their efficacy in tasks like cell type clustering and batch effect correction.   | Geneformer and scGPT exhibit inconsistent and often underwhelming performance in zero-shot settings for single-cell biology tasks like cell type clustering and batch effect correction, often falling behind simpler methods like scVI and highly variable gene selection. Pretraining these models on larger and more diverse datasets offers limited benefits, underscoring the need for more focused research to improve the robustness and utility of foundation models in single-cell biology.   |



### Review/Perspective Papers
| üìÑ Paper                                          |  üåü Highlights/Main Focus                       | üìù Remarks & Conclusion                |
|---------------------------------------------------|--------------------------------------------------|---------------------------------------|
| [Translating single-cell genomics into cell types](https://www.nature.com/articles/s42256-022-00600-6). Jesper N. Tegner. _Nature Machine Intelligence_ (2023)      | This paper emphasizes the successful adaptation of machine translation models, particularly transformers like BERT, for the task of cell type annotation in single-cell genomics. It highlights the development of scBERT, which leverages pretraining and self-supervised learning to create robust cell embeddings that are less sensitive to batch effects and capable of detecting subtle dependencies such as rare cell types.              | Despite demonstrating strong performance across diverse datasets and tasks, the paper acknowledges limitations, such as the need for embedding binning and the lack of integration with underlying biological processes like gene-regulatory networks. The authors suggest future research directions, including improving the generalization of embeddings to continuous values and developing more nuanced masking strategies. The paper concludes by noting the potential for transformers to be applied to other tasks in single-cell biology and anticipates growing interest in integrating AI methods beyond computer vision into bioinformatics and single-cell genomics. |

## DNA Models
Papers focused on the application of Transformer models in DNA sequence analysis.

### Original Papers
| üß† Model               | üìÑ Paper                                          | üíª Code              | üõ†Ô∏è Architecture           | üåü Highlights/Main Focus                          | üß¨ No. of Genomes | üìä No. of Datasets | üéØ Loss Function(s)       | üìù Downstream Tasks/Evaluations        |
|------------------------|---------------------------------------------------|----------------------|---------------------------|--------------------------------------------------|-----------------|-------------------|--------------------------|---------------------------------------|
| HyenaDNA üí°üîç       | [HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution](https://openreview.net/pdf?id=ubzNoJjOKj). _Conference on Neural Information Processing Systems (NeurIPS)_ (2023)  | [GitHub Repository](https://github.com/HazyResearch/hyena-dna)     | Transformer (Hyena)          | A genomic foundation model that leverages long-range context modeling using implicit convolutions, allowing it to process up to 1 million tokens at single nucleotide resolution.     | 1          | 1              | Cross-entropy loss          | 29 tasks, but mainly under single nucleotide resolution, in-context learning for genomic sequences, and ultralong-range genomics |
| DNABERT   | [DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome](https://academic.oup.com/bioinformatics/article/37/15/2112/6128680). _Bioinformatics_ (2023)  | [GitHub Repository](https://github.com/jerryji1993/DNABERT)   | Transformer (BERT)  | A pretrained BERT model adapted for DNA sequences that captures the complex regulatory code of genomes by leveraging upstream and downstream nucleotide contexts.  |  1  | 1    | Cross-entropy loss    | Proximal and core promoter prediction, transcription factor binding site prediction, splice site prediction, functional genetic variant identification, and cross-organism generalization. |
| GENA-LM                | [GENA-LM: A Family of Open-Source Foundational DNA Language Models for Long Sequences](https://github.com/AIRI-Institute/GENA_LM). _bioRxiv_ (2023) | [GitHub Repository](https://github.com/AIRI-Institute/GENA_LM)   | Transformer (BERT, BigBird)  | A suite of foundational DNA language modelsleveraging recurrent memory and sparse attention for long-range context modeling in genomic sequences. Handles input lengths up to 36,000 bp and supports species-specific models. |  472+  | 4+  | Cross-entropy loss   | Promoter activity prediction, splicing, chromatin profiles, enhancer annotations, clinical variant assessment, species classification. |
| GROVER                 | [GROVER: DNA Language Model Learns Sequence Context in the Human Genome](https://doi.org/10.1038/s42256-024-00872-0). _Nature Machine Intelligence_ (2024) | [Zenodo Repository](https://doi.org/10.5281/zenodo.8373202)   | Transformer (BERT)  | A DNA language model trained on the human genome, using byte-pair encoding for balanced token representation. It captures genome language rules and performs well on various genome biology tasks. |  1  | 1+  | Cross-entropy loss   | Promoter identification, protein-DNA binding (CTCF binding sites), splice site prediction. |
| SATORI                | [SATORI: A Self-Attention Model for Inferring Cooperativity Between Regulatory Features](https://doi.org/10.1093/nar/gkab349). _Nucleic Acids Research_ (2021) | [GitHub Repository](https://github.com/fahadahaf/satori)   | CNN + Self-Attention (optional RNN) | Detects cooperativity between regulatory elements using a self-attention mechanism, capable of identifying TF-TF interactions without expensive post-processing. | 164 cell lines (human) and 36 samples (Arabidopsis) | 4  | Cross-entropy, binary cross-entropy with logits | TF-TF interaction identification, chromatin accessibility prediction, motif extraction, gene regulation analysis. |
| GPN-MSA               | [GPN-MSA: An Alignment-Based DNA Language Model for Genome-Wide Variant Effect Prediction](https://doi.org/10.1101/2023.10.10.561776). _bioRxiv_ (2024) | [GitHub Repository](https://github.com/songlab-cal/gpn)   | Transformer (RoFormer)  | Alignment-based DNA language model using whole-genome multiple sequence alignments across 100 vertebrates, optimized for variant effect prediction. | Not specified  | 6+ | Weighted cross-entropy loss | Variant effect prediction, deleteriousness scoring, gene regulatory impact analysis. |
| C.Origami              | [Cell-type-specific prediction of 3D chromatin organization enables high-throughput in silico genetic screening](https://doi.org/10.1038/s41587-022-01612-8). _Nature Biotechnology_ (2023) | [GitHub Repository](https://github.com)   | Transformer + CNN  | Predicts cell-type-specific chromatin organization using DNA sequence, CTCF, and ATAC-seq data, enabling in silico genetic screening for 3D genome structure. | Not specified | 7 cell lines (human and mouse) | MSE, binary cross-entropy | Chromatin structure prediction, TAD boundary identification, genetic perturbation studies. |
| Nucleotide Transformer | [The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics](https://doi.org/10.1101/2023.01.11.523679). _bioRxiv_ (2024) | [GitHub Repository](https://github.com/instadeepai/nucleotide-transformer)   | Transformer (50M‚Äì2.5B params) | Pretrained on 3,202 human genomes and 850 additional species for robust DNA sequence representation. Scales from 50M to 2.5B parameters for comprehensive downstream applications. | 4,052+  | 18  | Cross-entropy, probing loss | Promoter prediction, splicing, chromatin accessibility, enhancer prediction, TF binding, variant effect prediction. |
| Borzoi                | [Predicting RNA-seq coverage from DNA sequence as a unifying model of gene regulation](https://doi.org/10.1101/2023.08.30.555582). _bioRxiv_ (2023) | [GitHub Repository](https://github.com)   | Transformer + Convolution + U-Net | Predicts RNA-seq coverage from DNA sequence to interpret regulatory variants impacting transcription, splicing, and polyadenylation. | Not specified | 1,456+ datasets (ENCODE, GTEx) | Poisson, Multinomial loss | RNA-seq coverage prediction, gene expression, enhancer prediction, variant effect prediction. |
| msBERT-Promoter       | [msBERT-Promoter: A Multi-Scale Ensemble Predictor Based on BERT Pre-trained Model for the Two-Stage Prediction of DNA Promoters and Their Strengths](https://doi.org/10.1186/s12915-024-01923-z). _BMC Biology_ (2024) | [GitHub Repository](https://github.com/liyazi712/msBERT-Promoter)   | BERT-based Ensemble  | Predicts promoter sequences and their strengths using multi-scale BERT-based ensemble with soft voting for improved accuracy. | Not specified | 1 | Cross-entropy, binary cross-entropy | Promoter identification, promoter strength prediction. |
| DNABERT-2             | [DNABERT-2: Efficient Foundation Model and Benchmark for Multi-Species Genomes](https://doi.org/10.1101/2023.06.15.150006). _International Conference on Machine Learning (ICLR)_ (2024) | [GitHub Repository](https://github.com/MAGICS-LAB/DNABERT_2)   | Transformer (BPE-based)  | Multi-species genome foundation model using BPE tokenization, enhancing efficiency and accuracy in genomic tasks. | 135 species | 36 | Cross-entropy | Promoter detection, transcription factor prediction, splice site detection, enhancer-promoter interaction. |
| Evo                   | [Sequence modeling and design from molecular to genome scale with Evo](https://doi.org/10.1101/2024.02.27.582234). _bioRxiv_ (2024) | [GitHub Repository](https://github.com/evo-design/evo)   | StripedHyena (Hybrid of Hyena and Transformer) | Scalable genomic model (7B params) capable of both generative and predictive tasks across molecular, systems, and genome scales, with 131k token context. | 80,000+ genomes | 10+ | Cross-entropy | Protein, ncRNA, and regulatory DNA prediction; gene essentiality; CRISPR-Cas and MGE generation. |
| BigBird               | [BigBird: Transformers for Longer Sequences](https://doi.org/10.48550/arXiv.2007.14062). _NeurIPS_ (2020) | [GitHub Repository](https://github.com/google-research/bigbird)   | Sparse Transformer  | Sparse attention mechanism enabling longer sequence handling with linear complexity, applied to genomics and NLP tasks. | Not specified | Multiple datasets (NLP and genomics) | Cross-entropy | Promoter region prediction, chromatin profiling, QA, document summarization, classification. |
| GeneBERT              | [Multi-Modal Self-Supervised Pre-Training for Regulatory Genome Across Cell Types](https://arxiv.org/abs/2110.05231). _bioRxiv_ (2021) | [GitHub Repository](https://github.com)   | BERT + Multi-modal Transformer | Pre-trained on multi-modal regulatory genome data with 1D and 2D modalities to capture cell-type variability in regulatory elements. | 17 million sequences | ATAC-seq (Descartes) | Cross-entropy, sequence-region matching | Promoter prediction, TF binding site prediction, disease risk estimation, splicing site prediction. |
| EBERT                | [Epigenomic language models powered by Cerebras](https://arxiv.org/abs/2112.07571). _arXiv_ (2021) | [GitHub Repository](https://github.com)   | BERT-based (with epigenetic states) | Incorporates epigenetic information alongside DNA sequences for better cell type-specific gene regulation modeling. Enabled by Cerebras CS-1 for efficient training. | 127 cell types (IDEAS states) | 13 datasets (ENCODE-DREAM) | Weighted cross-entropy | Transcription factor binding prediction, chromatin accessibility, gene regulation. |
| LOGO                  | [Integrating convolution and self-attention improves language model of human genome for interpreting non-coding regions at base-resolution](https://doi.org/10.1093/nar/gkac326). _Nucleic Acids Research_ (2022) | [GitHub Repository](https://github.com/melobio/LOGO)   | Transformer + Convolution  | Lightweight genome language model with convolution and self-attention layers, designed for base-resolution non-coding region interpretation. | Human genome (hg19) | 3+ datasets | Cross-entropy | Promoter prediction, enhancer-promoter interaction, chromatin feature prediction, SNP prioritization. |
| ViBE                  | [ViBE: a hierarchical BERT model to identify eukaryotic viruses using metagenome sequencing data](https://doi.org/10.1093/bib/bbac204). _Briefings in Bioinformatics_ (2022) | [GitHub Repository](https://github.com/DMnBI/ViBE)   | Hierarchical BERT  | Hierarchical model to classify eukaryotic viral taxa using domain-level and order-level classification with metagenomic sequencing data. | 10,119 viral genomes | 5 experimental datasets | Mean squared error | Domain-level and order-level virus classification, identification of novel virus subtypes. |
| FloraBERT             | [FloraBERT: Cross-species transfer learning with attention-based neural networks for gene expression prediction](https://doi.org/10.21203/rs.3.rs-1927200/v1). _Research Square_ (2022) | [GitHub Repository](https://github.com/benlevyx/florabert)   | BERT-based Transformer | Cross-species gene expression prediction using plant genome assemblies, leveraging attention for regulatory motif learning. | 93 plant genomes | 3 databases (Ensembl, RefSeq, MaizeGDB) | Cross-entropy | Gene expression prediction, promoter analysis, phylogenetic feature extraction. |
| INHERIT               | [Identification of bacteriophage genome sequences with representation learning](https://doi.org/10.1093/bioinformatics/btac509). _Bioinformatics_ (2022) | [GitHub Repository](https://github.com/Celestial-Bai/INHERIT)   | DNABERT-based Transformer  | Combines database-based and alignment-free approaches for phage identification using a pre-trained DNABERT model. | 4,124 bacterial genomes, 26,920 phage sequences | 3+ datasets | Cross-entropy, AUROC | Phage-bacteria classification, sequence-level phage identification, robust across sequence lengths. |
| GenSLMs               | [GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics](https://doi.org/10.1101/2022.10.10.511571). _bioRxiv_ (2022) | [GitHub Repository](https://github.com/ramanathanlab/genslm)   | Hierarchical Transformer + Diffusion Model | Trained on 110M prokaryotic gene sequences and fine-tuned on 1.5M SARS-CoV-2 genomes for variant detection and evolutionary analysis. | 1.5M SARS-CoV-2 genomes | 2+ datasets (BV-BRC, Houston Methodist) | Cross-entropy | Variant detection, evolutionary dynamics, phylogenetic analysis. |
| SpliceBERT            | [Self-supervised learning on millions of primary RNA sequences from 72 vertebrates improves sequence-based RNA splicing prediction](https://doi.org/10.1093/bib/bbae163). _Briefings in Bioinformatics_ (2024) | [GitHub Repository](https://github.com/biomed-AI/SpliceBERT)   | BERT-based Transformer | Pretrained on RNA sequences from 72 vertebrates for evolutionary conservation and RNA splicing predictions. | 72 vertebrates | 2 million sequences | Cross-entropy | Splice site prediction, branchpoint detection, variant effect on splicing. |
| SpeciesLM | [Species-aware DNA language models capture regulatory elements and their evolution](https://doi.org/10.1186/s13059-024-03221-x). _Genome Biology_ (2024) | [GitHub Repository](https://github.com/gagneurlab/SpeciesLM)   | DNABERT-based Transformer | Trained on 806 fungal species across 500 million years, identifying conserved regulatory elements and their evolution in non-coding DNA sequences. | 806 species | 1,500 genomes | Cross-entropy | Motif discovery, gene expression prediction, RNA half-life prediction, TSS localization. |
| DNAGPT                | [DNAGPT: A Generalized Pre-trained Tool for Versatile DNA Sequence Analysis Tasks](https://arxiv.org/abs/2307.05628). _bioRxiv_ (2023) | [GitHub Repository](https://github.com/tencent-ailab/DNAGPT)   | Transformer-based GPT | Trained on over 200 billion base pairs from mammalian genomes; supports multi-task DNA sequence and numerical data analysis for various downstream applications. | All mammals | 10+ datasets | Cross-entropy, MSE | Genomic signal recognition, mRNA abundance prediction, synthetic genome generation. |
| UTR-LM                | [A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions](https://doi.org/10.1101/2023.10.11.561938). _bioRxiv_ (2023) | [GitHub Repository](https://github.com/a96123155/UTR-LM)   | Transformer-based | Predicts regulatory functions of 5' UTRs, including ribosome loading, translation efficiency, and expression level using multi-modal learning. | 5 species | 3+ datasets | Cross-entropy, MSE | Mean ribosome loading, translation efficiency, mRNA expression level prediction, IRES identification. |
| hgT5                  | [GUANinE v1.0: Benchmark Datasets for Genomic AI Sequence-to-Function Models](https://doi.org/10.1101/2023.10.12.562113). _bioRxiv_ (2023) | [GitHub Repository](https://github.com/ni-lab/guanine)   | T5-based Transformer | Designed for functional genomics, focusing on DNase hypersensitivity, cCRE annotation, conservation, and gene expression prediction in yeast and human genomes. | Human, yeast | 6+ tasks | Cross-entropy, MSE, Spearman rho | DNase hypersensitivity, regulatory element annotation, conservation scoring, gene expression prediction. |
| AgroNT                | [A Foundational Large Language Model for Edible Plant Genomes](https://doi.org/10.1101/2023.10.24.563624). _bioRxiv_ (2024) | [HuggingFace Repository](https://huggingface.co/InstaDeepAI/agro-nucleotide-transformer-1b) | Transformer-based | Trained on genomes from 48 edible plant species, focusing on regulatory annotations, tissue-specific gene expression, and functional variant prioritization. | 48 plant species | 8 tasks | Cross-entropy, regression loss | Promoter strength prediction, gene expression prediction, enhancer annotation, variant effect prediction. |
| megaDNA               | [Transformer Model Generated Bacteriophage Genomes are Compositionally Distinct from Natural Sequences](https://doi.org/10.1101/2024.03.19.585716). _bioRxiv_ (2024) | [GitHub Repository](https://github.com/lingxusb/megaDNA)   | MEGABYTE Transformer | Generates synthetic bacteriophage genomes, showing compositional differences from natural sequences, useful for biosecurity analysis. | 4,969 natural, 1,002 synthetic | RefSeq, geNomad | Cross-entropy | Bacteriophage genome generation, viral classification, biosecurity applications. |
| ChatNT                | [ChatNT: A Multimodal Conversational Agent for DNA, RNA and Protein Tasks](https://doi.org/10.1101/2024.04.30.591835). _bioRxiv_ (2024) | [GitHub Repository](https://github.com/InstaDeepAI/ChatNT)   | DNA encoder (Nucleotide Transformer) + English decoder (Vicuna-7B) | Multimodal conversational agent for biological sequences, achieving state-of-the-art results in DNA, RNA, and protein tasks using a unified framework. | 500M tokens (Nucleotide Transformer benchmark) | 27 genomics tasks | Cross-entropy | Promoter prediction, enhancer activity, RNA degradation, protein stability, gene expression prediction. |
| LucaOne               | [LucaOne: Generalized Biological Foundation Model with Unified Nucleic Acid and Protein Language](https://doi.org/10.1101/2024.05.10.592927). _bioRxiv_ (2024) | [GitHub Repository](https://github.com/alibaba/LucaOne)   | Transformer-Encoder | Unified model for nucleic acid and protein sequences, trained on data from 169,861 species to support DNA, RNA, and protein tasks across multiple bioinformatics applications. | 169,861 species | 10 datasets | Cross-entropy, MSE | Gene expression prediction, protein stability, ncRNA family classification, genus taxonomy, protein-protein interactions, RNA-protein interactions. |
| CD-GPT                | [CD-GPT: A Biological Foundation Model Bridging the Gap between Molecular Sequences Through Central Dogma](https://doi.org/10.1101/2024.06.24.600337). _bioRxiv_ (2024) | [GitHub Repository](https://github.com/TencentAI4S/CD-GPT)   | Transformer-based GPT | Unified model for DNA, RNA, and protein sequences, pretrained on multi-molecular data to capture central dogma relationships. | 14 species | 353 million sequences | Cross-entropy | Promoter detection, splice site prediction, solubility prediction, RNA-protein interaction, protein generation. |
| SpeciesLM             | [Nucleotide dependency analysis of DNA language models reveals genomic functional elements](https://doi.org/10.1101/2024.07.27.605418). _bioRxiv_ (2024) | [GitHub Repository](https://github.com)   | Transformer with species-aware tokenization | Analyzes nucleotide dependencies in genomic sequences to identify regulatory elements, RNA structural contacts, and transcription factor motifs across species. | 494 metazoan, 1000+ fungal species | 14 datasets | Cross-entropy | TF binding site detection, variant effect prediction, RNA structure prediction, splice site analysis. |





### Benchmarking Papers
| üìÑ Paper                                          | üíª Code              | üß† Benchmarking Models           | üåü Main Focus                          | üìù Results & Insights |
|---------------------------------------------------|----------------------|----------------------------------|----------------------------------------|-----------------------|
| [BEND: Benchmarking DNA Language Models on biologically meaningful tasks](https://arxiv.org/abs/2311.12570). Frederikke Isa Marin et al. _arXiv_ (2024) |  [GitHub Repository](https://github.com/frederikkemarin/BEND)   | AWD-LSTM, Dilated ResNet, Nucleotide Transformer (NT-MS, NT-V2, NT-1000G), DNABERT, DNABERT-2, GENA-LM (BERT, BigBird), HyenaDNA (large, small), GROVER, and Basset | The paper introduces BEND, a benchmark designed to evaluate DNA language models (LMs) using realistic, biologically meaningful tasks on the human genome. BEND includes seven tasks that assess the models' ability to capture functional elements across various length scales. |The main results of the BEND benchmark reveal that DNA language models (LMs) show promising but mixed performance across different tasks. Nucleotide Transformer (NT-MS) performed best overall, particularly in gene finding, histone modification, and CpG methylation tasks. DNABERT excelled in chromatin accessibility prediction, matching the performance of the Basset model. However, no model consistently outperformed all others, and long-range tasks like enhancer annotation remained challenging for all models. The study highlighted the need for further improvement in capturing long-range dependencies in genomic data. |


### Review/Perspective Papers
| üìÑ Paper                                          |  üåü Highlights/Main Focus                       | üìù Remarks & Conclusion                |
|---------------------------------------------------|--------------------------------------------------|---------------------------------------|
| [To Transformers and Beyond: Large Language Models for the Genome](https://arxiv.org/abs/2311.07621). Micaela E. Consens et al. _arXiv_ (2024)      | This paper explores the revolutionary impact of Large Language Models (LLMs) on genomics, focusing on their capacity to tackle the complexities of DNA, RNA, and single-cell sequencing data. By adapting the transformer architecture, traditionally used in natural language processing, LLMs offer a novel approach to uncover genomic patterns, predict functional elements, and enhance genomic data interpretation. The review delves into transformer-hybrid models and emerging architectures beyond transformers, outlining their applications, benefits, and limitations in genomic data analysis. The goal is to bridge gaps between computational biology and machine learning in the evolving field of genomics. | The paper emphasizes that while transformer-based LLMs have significantly advanced genomic modeling, challenges like scaling to larger contexts and maintaining interpretability remain. Innovations such as the Hyena layer promise to address computational inefficiencies, further pushing the boundaries of genomic data analysis. Future research should focus on improving context length, integrating multi-omic data, and refining interpretability to fully realize the potential of LLMs. Overall, the review highlights the transformative potential of these models in genomics, pointing toward an exciting future for computational biology. |
| [Genomic Language Models: Opportunities and Challenges](https://arxiv.org/abs/2407.11435). Gonzalo Benegas et al. _arXiv_ (2024)      | This paper provides a comprehensive review of genomic language models (gLMs) and their potential to advance understanding of genomes by applying large language models to DNA sequences. Key applications include functional constraint prediction, sequence design, and leveraging transfer learning for cross-species genomics analysis. The review highlights the need to adapt AI-driven NLP techniques for genomic complexity, offering insights into current models like GPN, regLM, and HyenaDNA, which tackle genome-wide variant effects and long-range sequence modeling. | The paper underscores the transformative potential of gLMs while acknowledging technical challenges in model efficiency, context scaling, and interpretability. Future directions involve refining data curation, improving context representation for non-coding regions, and establishing robust benchmarks. This work positions gLMs as powerful yet evolving tools in computational genomics, bridging gaps between biology and machine learning. |




 
## Spatial Transcriptomics (ST) Models
Papers applying Transformer models to spatial transcriptomics data.

### Original Papers
| üß† Model               | üìÑ Paper                                          | üíª Code              | üõ†Ô∏è Architecture           | üåü Highlights/Main Focus                          | üß¨ No. of Cells | üìä No. of Datasets | üéØ Loss Function(s)       | üìù Downstream Tasks/Evaluations        |
|------------------------|---------------------------------------------------|----------------------|---------------------------|--------------------------------------------------|-----------------|-------------------|--------------------------|---------------------------------------|
| Nicheformer üí°           | [Nicheformer: A Foundation Model for Single-Cell and Spatial Omics](https://www.biorxiv.org/content/10.1101/2024.04.15.589472v1). Anna C. Schaar et al. _bioRxiv_ (2024)    | [GitHub Repository](https://github.com/theislab/nicheformer)                     | Transformer               | Transformer-based model that integrates over 110 million human and mouse cells, learning unified representations from dissociated and spatial transcriptomics for advanced analysis of cellular interactions and environments. | 110M     | 180+      | Masked language modeling loss, Cross entropy loss | Spatial cell type, niche region label prediction; Neighborhood cell density prediction |
| SpaFormer              | [Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Denoising](https://doi.org/10.48550/arXiv.2302.03038v2). _bioRxiv_ (2024) | [GitHub Repository](https://github.com/wehos/CellT)   | Transformer (Performer) | Transformer-based model leveraging positional encodings for spatial transcriptomic data denoising and imputation. Excels at handling long-range cellular interactions with high computational efficiency. |  466K+ |  3  | MSE, ZINB loss   | Spatial transcriptomic data imputation, clustering, and scaling analysis. |
| TransformerST          | [Innovative super-resolution in spatial transcriptomics: a transformer model exploiting histology images and spatial gene expression](https://doi.org/10.1093/bib/bbae052). _Briefings in Bioinformatics_ (2024) | [GitHub Repository](https://github.com/Zhaocy-Research/TransformerST)   | Transformer (Vision + Graph Transformer)  | A model enhancing spatial transcriptomics by incorporating histology images and gene expression data for single-cell resolution without scRNA-seq references. | Not specified  | 4+ | Reconstruction loss (gene, image), KL divergence | Tissue identification, super-resolution (single-cell), clustering, gene expression prediction. |
| SiGra                  | [SiGra: Single-cell spatial elucidation through an image-augmented graph transformer](https://doi.org/10.1038/s41467-023-41437-w). _Nature Communications_ (2023) | [GitHub Repository](https://github.com/QSong-github/SiGra)   | Graph Transformer  | A hybrid graph transformer leveraging single-cell spatial graphs with multimodal data for spatial domain identification and gene expression enhancement. |  83K+ | 3+  | MSE, combined self-supervised loss   | Spatial domain identification, data denoising, intratumor heterogeneity characterization, intercellular communication analysis. |
| Hist2ST                | [Hist2ST: Spatial Transcriptomics Prediction from Histology Jointly Through Transformer and Graph Neural Networks](https://doi.org/10.1093/bib/bbac297). _Briefings in Bioinformatics_ (2022) | [GitHub Repository](https://github.com/biomed-AI/Hist2ST)   | Transformer + Graph Neural Network  | Predicts spatial gene expression from histology images, leveraging 2D vision features, spatial dependency, and self-distillation. | Not specified  | 8+  | ZINB loss, MSE | Gene expression prediction, spatial region identification, clustering. |
| THItoGene              | [THItoGene: A deep learning method for predicting spatial transcriptomics from histological images](https://doi.org/10.1093/bib/bbad464). _Briefings in Bioinformatics_ (2024) | [GitHub Repository](https://github.com/yrjia1015/THItoGene)   | CNN, Capsule Network, ViT, GAT  | Predicts spatial gene expression from histopathological images, utilizing dynamic convolution and capsule networks for enhanced prediction accuracy. | Not specified  | 2+ | MSE, ZINB loss, PCC | Gene expression prediction, spatial domain identification, tumor-associated gene detection. |
| CellPLM üí°üîç        | [CellPLM: Pre-training of Cell Language Model Beyond Single Cells](https://openreview.net/pdf?id=BKXvPDekud). Hongzhi Wen et al. _International Conference on Learning Representations (ICLR)_ (2024) |       [GitHub Repository](https://github.com/OmicsML/CellPLM)   | Transformer | The framework marks the first of its kind, encoding inter-cell relations, harnessing spatially-resolved transcriptomic data, and adopts a decent prior distribution. | 9M scRNA-seq + 2M spatial | 3 | Masked language modeling with mean squared error loss | Zero-shot clustering; scRNA-seq denoising; Spatial transcriptomic imputation; Cell type annotation; Perturbation prediction |
| M2ORT                 | [M2ORT: Many-To-One Regression Transformer for Spatial Transcriptomics Prediction from Histopathology Images](https://doi.org/10.48550/arXiv.2401.10608v2). _arXiv_ (2024) | [GitHub Repository](https://github.com/Dootmaan/M2ORT)   | Transformer  | Multi-scale many-to-one regression transformer that leverages multiple pathology images to predict gene expressions for spatial transcriptomics. | Not specified  | 3 | MSE, PCC | Spatial transcriptomics prediction, gene expression prediction, cancer subtype analysis. |
| stEnTrans     | [stEnTrans: Transformer-based deep learning for spatial transcriptomics enhancement](https://github.com/shuailinxue/stEnTrans). Shuailin Xue et al. _bioRxiv_ (2024) | [GitHub Repository](https://github.com/shuailinxue/stEnTrans) | Transformer                 | Self-supervised model that enhances gene expression in unmeasured tissue areas, with superior accuracy and resolution.       | Not specified      | 6                 | Mean Squared Error   | Gene expression interpolation, spatial pattern discovery, biological pathway enrichment analysis             |
| SpaDiT        | [SpaDiT: Diffusion Transformer for Spatial Gene Expression Prediction using scRNA-seq](https://github.com/lllxxyyy-lxy/SpaDiT). Xiaoyu Li et al. _Briefings in Bioinformatics_ (2024) | [GitHub Repository](https://github.com/lllxxyyy-lxy/SpaDiT) | Transformer-based diffusion model | Integrates scRNA-seq data for spatial transcriptomics gene prediction, achieving state-of-the-art accuracy and maintaining spatial gene expression patterns |  Not specified   | 10                | MSE, JS divergence, Structural Similarity Index Measure (SSIM) | Spatial gene expression prediction, spatial pattern restoration, robustness analysis across data sparsity |
| stFormer      | [stFormer: a foundation model for spatial transcriptomics](https://doi.org/10.1101/2024.09.27.615337). Shenghao Cao et al. _bioRxiv_ (2024) | [GitHub Repository](https://github.com/csh3/stFormer) | Transformer with cross-attention for ligand-receptor info | Integrates ligand-receptor interaction data for better spatial gene clustering, hierarchy and membership encoding in gene networks | ~580K | 2                 | Mean Squared Error (MSE) | Cell-type clustering, ligand-receptor interaction inference, receptor-dependent gene network analysis, in silico perturbation simulation |









### Benchmarking Papers
| üìÑ Paper                                          | üíª Code              | üß† Benchmarking Models           | üåü Main Focus                          | üìù Results & Insights |
|---------------------------------------------------|----------------------|----------------------------------|----------------------------------------|-----------------------|
| [x](#)      | [x](#)     | [x](#) | x    | x |

### Review/Perspective Papers
| üìÑ Paper                                          |  üåü Highlights/Main Focus                       | üìù Remarks & Conclusion                |
|---------------------------------------------------|--------------------------------------------------|---------------------------------------|
| [x](#)     | x               | x |

## Hybrids of SCG, DNA, and ST Models
Papers that combine approaches and modalities from SCG, DNA, and ST using Transformers.

### Original Papers
| üß† Model               | üî¨ Omic Input Modalities     | üí° Pre-training | üìä Data, Cells, Tissues, Species                          | üîó Tokenization/Encoding                                  | üß© Input Embedding                           | üõ†Ô∏è Architecture                                | üéØ Output Trained to Prediction/Data-Integration                | üöÄ Zero Shot Tasks                                            | üîç Interpretation Method                      |
|------------------------|-----------------------------|-----------------|---------------------------------------------------------|---------------------------------------------------------|--------------------------------------------|-------------------------------------------------------------|-----------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------|
| AgroNT üí°üîç            | DNA sequences               | Yes             | Pretraining: ~10.5M sequences across 48 plant species; Fine-tuning: 8 tasks | Non-overlapping 6-mers (6000 bp chunks, 15% masked for MLM) | 1500-dimensional embeddings (token + positional embeddings) | Transformer, 40 attention blocks, 1B parameters              | Predict polyadenylation sites, splicing, chromatin accessibility, tissue-specific expression | Functional variant impacts, tissue expression variance       | Token importance, LLR, in silico mutagenesis  |
| Evo üí°üîç               | DNA sequences               | Yes             | 300B nucleotides, 2.7M genomes, 80K prokaryotic species  | Byte-level single-nucleotide tokenization; context length 131 kb | StripedHyena with rotary position embeddings  | StripedHyena: 29 convolution + 3 attention layers             | Mutation effects, CRISPR systems, transposons                | Fitness effects prediction, regulatory sequence generation   | Attention weights, positional entropy         |
| gLM2 üí°                | DNA sequences               | Yes             | OMG dataset: 3.1T bp, 3.3B CDS, 2.8B IGS                 | CDS: amino acids; IGS: nucleotides; strand orientation tokens | 640‚Äì1280 dimensions, RoPE positional embeddings | Transformer-based, SwiGLU layers, FlashAttention-2            | Protein-protein interactions, regulatory annotations          | Binding interface prediction, motif learning                | Categorical Jacobian, UMAP                    |
| MarkerGeneBERT üí°üîç    | scRNA-seq                   | Yes             | 3702 studies; 7901 markers for humans, 8223 for mice     | Tokenized marker sentences; SciBERT preprocessing        | Sentence embeddings, SciBERT refinements     | Transformer-based NLP with SciBERT                          | Extract cell markers, annotate scRNA-seq                      | Predict novel markers, cluster annotation                   | Attention weights, precision-recall           |
| UTR-LM üí°üîç            | 5‚Ä≤ UTRs of mRNA             | Yes             | 214k UTRs (5 species), 280k synthetic libraries          | Masked nucleotide prediction                            | 128-dimensional nucleotide embedding         | Six-layer transformer, 16 attention heads                    | MRL, TE, EL, IRES prediction                                 | Luciferase fitness, unseen UTR prediction                   | Motif analysis, UMAP                        |
| scGPT üí°üîç             | scRNA-seq                   | Yes             | 33M human cells, 441 studies, 51 tissues/organs          | Gene expression ranked encoding, metadata tokens        | 512-dimensional gene-cell embeddings         | 12-layer transformer, masked multi-head attention            | Cell type annotation, batch correction                       | Perturbation prediction, multi-omics integration            | Attention weights, UMAP visualization         |
| ScRAT üí°               | scRNA-seq                   | No              | COVID-19 datasets (~50 samples, 200‚Äì2500 cells/sample)   | Normalized UMI counts; mixup module                     | PCA-reduced 50 dimensions                     | Attention-based NN, 8-heads, 1-layer MLP                     | COVID phenotype prediction                                   | Cell population phenotype relevance                        | Attention weights, R-Score                   |
| THItoGene              | Histological images        | No              | HER2+ breast cancer (32 sections, 9,612 spots, 785 genes) | Spots tokenized via positional encoding; 112√ó112 patches for histology | Dynamic convolution with ViT and GAT integration | Hybrid: dynamic convolution, Efficient-CapsNet, ViT, GAT    | Spatial gene expression patterns, tumor-related gene identification | Reconstruct spatial domains, predict enrichment in unseen tissues | Attention weights, ARI clustering, Pearson correlation |
| scTranslator           | scRNA-seq                  | Yes             | Bulk datasets (31 cancer types, 18,227 samples), Single-cell datasets (161,764 PBMCs, 65,698 pan-cancer myeloid cells) | Gene IDs via re-indexed GPE; RNA expression values as tokens | 128-dim GPE embeddings + RNA embeddings      | Transformer encoder-decoder, 2 layers, FAVOR+ attention      | Protein abundance prediction, batch correction, pseudo-knockout analysis | Predict missing proteomics, tumor/normal cell origins      | Attention matrices, pseudo-knockout analysis, ARI clustering |
| GPN-MSA                | DNA sequences              | Yes             | Whole-genome MSA of 100 vertebrates (~9B variants)       | One-hot encoding across MSA columns; weighted token sampling | Contextual embeddings from MSA               | 12-layer Transformer with RoFormer; weighted cross-entropy loss | Variant deleteriousness scores, novel region annotation      | Predict deleterious variants, annotate non-coding regions   | UMAP, phastCons/phyloP correlation, epigenetic enrichment |
| FloraBERT              | Plant DNA sequences        | Yes             | ~7.9M plant promoters (93 species); maize fine-tuning (25 genomes, 9 tissues) | Byte Pair Encoding (5,000-token vocabulary)            | 768-dim token + positional embeddings         | RoBERTa-based Transformer, 6 encoder layers, 6 attention heads | Gene expression prediction across tissues                   | Regulatory potential in unseen species, cross-species similarity | Positional importance, UMAP embedding visualization, R¬≤ metrics |
| Enformer               | DNA sequences              | Yes             | Human genome (34k training, 2k validation), mouse genome (29k training) | One-hot nucleotide encoding, spatial positional encodings | Convolutional embedding for initial sequence processing | 7 convolutional layers + 11 transformer layers               | Gene expression, enhancer-promoter interactions, variant effects | Variant prioritization, enhancer-gene annotation           | Attention weights, SLDP, gradient √ó input for impact      |
















### Benchmarking Papers
| üìÑ Paper                                          | üíª Code              | üß† Benchmarking Models           | üåü Main Focus                          | üìù Results & Insights |
|---------------------------------------------------|----------------------|----------------------------------|----------------------------------------|-----------------------|
| [x](#)      | [x](#)     | [x](#) | x    | x |

### Review/Perspective Papers
| üìÑ Paper                                          |  üåü Highlights/Main Focus                       | üìù Remarks & Conclusion                |
|---------------------------------------------------|--------------------------------------------------|---------------------------------------|
| [x](#)      | x               | x |


